{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import requests\n",
    "import re\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from time import sleep\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.support.ui import Select\n",
    "from fake_useragent import UserAgent"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# 針對新版\n",
    "def get_elements_new(year, url_dict):\n",
    "    error_articles = []\n",
    "    output_data = []\n",
    "    headers = {\"user-agent\":\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/93.0.4577.63 Safari/537.36\"}\n",
    "    \n",
    "    # 從url_dict中取得LRN, Url => 取得LRN號碼用意為錯誤發生時可知道是哪篇無法爬取\n",
    "    for LRN, url in url_dict.items():\n",
    "        print(LRN)\n",
    "        \n",
    "        # 建立beautifulsoup物件\n",
    "        ele_url = url\n",
    "        ele_res = requests.get(ele_url, headers=headers)\n",
    "        ele_soup = BeautifulSoup(ele_res.text, features=\"html.parser\")\n",
    "\n",
    "        # 取得 title, release_num, time, extra_info, content => use try-except to filter the error message\n",
    "        try:\n",
    "            title = ele_soup.select(\"h1.alphaheads\")[0].text\n",
    "            subtitle_data = ele_soup.select(\"h2.alphaheads\")\n",
    "            release_num, time = subtitle_data[0].text.split(\"/\")\n",
    "            extra_info = \",\".join([i.text for i in subtitle_data[1:]])\n",
    "            content_data = ele_soup.select('div[id=main-content]')\n",
    "            content = \"\\n\".join([i.text for i in content_data[0].select(\"p\")])\n",
    "\n",
    "            # 將每篇article資料放入輸出資料，待會轉成dataframe\n",
    "            output_data.append([title, release_num, time, extra_info, content])\n",
    "\n",
    "        # 跳過indexerror => append進入error_articles、回傳LRN編號\n",
    "        except (IndexError, ValueError) as e:\n",
    "            error_articles.append((LRN, url))\n",
    "            print(\"有問題無法爬取 => \", LRN)\n",
    "        \n",
    "        sleep(1)\n",
    "    # 將當年度error_article依年份存入whole_error_articles中        \n",
    "    #whole_error_articles[year] = error_articles\n",
    "    \n",
    "    return output_data"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# 針對舊版\n",
    "def get_elements_old(year, url_dict):\n",
    "    error_articles = []\n",
    "    output_data = []\n",
    "    ua = UserAgent()\n",
    "    headers = {\"user-agent\": ua.google} # 每年用不同的user-agent去爬\n",
    "\n",
    "    # 從url_dict中取得LRN, Url => 取得LRN號碼用意為錯誤發生時可知道是哪篇無法爬取\n",
    "    for LRN, url in url_dict.items():\n",
    "        print(LRN)\n",
    "\n",
    "        # 建立beautifulsoup物件\n",
    "        ele_res = requests.get(url, headers=headers)\n",
    "        ele_soup = BeautifulSoup(ele_res.text, features=\"html.parser\")\n",
    "\n",
    "        # 取得 title, release_num, time, extra_info, content => use try-except to filter the error message\n",
    "        try:\n",
    "            # 取得基本情報 => 幾乎都存放在h開頭的標籤裡面，透過for loop進行蒐集tag之後，從soup中extract出來，避免擷取內文時被再次抓到（2011以前都是如此）\n",
    "            raw_data = []\n",
    "            for i in range(1, 4):\n",
    "                data = ele_soup.select(f\"h{i}\")\n",
    "                if data:\n",
    "                    for d in data:\n",
    "                        raw_data.append(d.extract())\n",
    "\n",
    "            # 從tag中取出文字\n",
    "            elements = list(map(lambda x: x.text, raw_data))\n",
    "\n",
    "            # 流程：將必要資訊拿出後，從list裡面remove掉，到最後剩下的element組成titile(無規則性，後續再進行文字處理)\n",
    "            # 1. 找出不必要資訊並刪除\n",
    "            useless = [i for i in elements if re.match(\".*Securities and Exchange Commission.*\", i, flags=re.I)][0]\n",
    "            elements.remove(useless)\n",
    "            # print(elements)\n",
    "\n",
    "            # 2. 號碼、日期\n",
    "            result = [i for i in elements if re.match(\"^Litigation.*\", i, flags=re.I)][0]\n",
    "            elements.remove(result)\n",
    "            release_num, date = result.split(\"/\")\n",
    "\n",
    "            # 3. extra info\n",
    "            result = [i for i in elements if re.match(\"^SEC v\\..*|^Securities and Exchange Commission v\\..*\", i, flags=re.I)][0]\n",
    "            elements.remove(result)\n",
    "            extra_info = result\n",
    "\n",
    "            # 4. 標題\n",
    "            title = \" \".join(elements)\n",
    "\n",
    "            # 5. 取得內文 => 皆放在p標籤裡面，抓取全部並用join組成一個大字串，再以特定句進行split\n",
    "            articles = \" \".join([i.text for i in ele_soup.select(\"p\")])\n",
    "\n",
    "            if \"For further information\" in articles:\n",
    "                content = articles.split(r\"For further information\")[0]\n",
    "            elif \"SEC Complaint\" in articles:\n",
    "                content = articles.split(\"SEC Complaint\")[0]\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "            # 將每篇article資料放入輸出資料，待會轉成dataframe\n",
    "            output_data.append([title, release_num, date, extra_info, content])\n",
    "\n",
    "        # 跳過indexerror => append進入error_articles、回傳LRN編號\n",
    "        except (IndexError, ValueError) as e:\n",
    "            error_articles.append((LRN, url))\n",
    "            print(\"有問題無法爬取 => \", LRN)\n",
    "        \n",
    "    # 將當年度error_article依年份存入whole_error_articles中        \n",
    "    whole_error_articles[year] = error_articles\n",
    "    \n",
    "    return output_data"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 568,
   "source": [
    "# 舊版 - 單頁元素抓取\n",
    "def single_page_element(url):\n",
    "    output_data = []\n",
    "        \n",
    "    # 建立beautifulsoup物件\n",
    "    # headers = {\"user-agent\":\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/93.0.4577.63 Safari/537.36\"}\n",
    "    ua = UserAgent()\n",
    "    headers = {\"user-agent\": ua.google}\n",
    "    ele_res = requests.get(url, headers=headers)\n",
    "    ele_soup = BeautifulSoup(ele_res.text, features=\"html.parser\")\n",
    "\n",
    "    # 取得 title, release_num, time, extra_info, content => use try-except to filter the error message\n",
    "    try:\n",
    "        # 取得基本情報 => 幾乎都存放在h開頭的標籤裡面，透過for loop進行蒐集tag\n",
    "        raw_data = []\n",
    "        for i in range(1, 4):\n",
    "            data = ele_soup.select(f\"h{i}\")\n",
    "            if data:\n",
    "                for d in data:\n",
    "                    raw_data.append(d.extract())\n",
    "\n",
    "        # 從tag中取出文字\n",
    "        elements = list(map(lambda x:x.text, raw_data))\n",
    "\n",
    "        # 流程：將必要資訊拿出後，從list裡面remove掉，到最後剩下的element組成titile(無規則性，後續再進行文字處理)\n",
    "        # 1. 找出不必要資訊並刪除\n",
    "        useless = [i for i in elements if re.match(\".*Securities and Exchange Commission.*\", i, flags=re.I)][0]\n",
    "        elements.remove(useless)\n",
    "        # print(elements)\n",
    "\n",
    "        # 2. 號碼、日期\n",
    "        result = [i for i in elements if re.match(\"^Litigation.*\", i, flags=re.I)][0]\n",
    "        elements.remove(result)\n",
    "        release_num, date = result.split(\"/\")\n",
    "\n",
    "        # 3. extra info\n",
    "        result = [i for i in elements if re.match(\"^SEC v\\..*|^Securities and Exchange Commission v\\..*\", i, flags=re.I)][0]\n",
    "        elements.remove(result)\n",
    "        extra_info = result\n",
    "\n",
    "        # 4. 標題\n",
    "        title = \" \".join(elements)\n",
    "\n",
    "        # 取得內文 => 皆放在p標籤裡面，抓取全部並用join組成一個大字串，再以特定句進行split\n",
    "        articles = \" \".join([i.text for i in ele_soup.select(\"p\")])\n",
    "\n",
    "        if \"For further information\" in articles:\n",
    "            content = articles.split(r\"For further information\")[0]\n",
    "        elif \"SEC Complaint\" in articles:\n",
    "            content = articles.split(\"SEC Complaint\")[0]\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "        # 將每篇article資料放入輸出資料，待會轉成dataframe\n",
    "        output_data.append([title, release_num, date, extra_info, content])\n",
    "\n",
    "    except:\n",
    "        print(\"error happened\")\n",
    "    \n",
    "    return output_data"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 564,
   "source": [
    "url = \"https://www.sec.gov/litigation/litreleases/lr18136.htm\" #\"https://www.sec.gov/litigation/litreleases/2011/lr22213.htm\"\n",
    "final_result  = single_page_element(url)\n",
    "\n",
    "df = pd.DataFrame(final_result, columns=[\"title\", \"release_num\", \"time\", \"extra_info\", \"content\"])\n",
    "df"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "error happened\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [title, release_num, time, extra_info, content]\n",
       "Index: []"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>release_num</th>\n",
       "      <th>time</th>\n",
       "      <th>extra_info</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 564
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 567,
   "source": [
    "#### 測試！！！！\n",
    "url = \"https://www.sec.gov/litigation/litreleases/lr18322.htm\" #\"https://www.sec.gov/litigation/litreleases/2017/lr23994.htm\"\n",
    "\n",
    "headers = {\"user-agent\":\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/93.0.4577.63 Safari/537.36\"}\n",
    "ele_res = requests.get(url, headers=headers)\n",
    "ele_soup = BeautifulSoup(ele_res.text, features=\"html.parser\")\n",
    "\n",
    "# 取得基本情報 => 幾乎都存放在h開頭的標籤裡面，透過for loop進行蒐集tag\n",
    "raw_data = []\n",
    "for i in range(1, 4):\n",
    "    data = ele_soup.select(f\"h{i}\")        \n",
    "    if data:\n",
    "        raw_data.extend(data)\n",
    "\n",
    "# 從tag中取出文字\n",
    "elements = list(map(lambda x:x.text, raw_data))\n",
    "\n",
    "# 流程：將必要資訊拿出後，從list裡面remove掉，到最後剩下的element組成titile(無規則性，後續再進行文字處理)\n",
    "# 1. 找出不必要資訊並刪除\n",
    "useless = [i for i in elements if re.match(\".*Securities and Exchange Commission.*\", i, flags=re.I)][0]\n",
    "elements.remove(useless)\n",
    "print(elements)\n",
    "\n",
    "# 2. 號碼、日期\n",
    "result = [i for i in elements if re.match(\"^Litigation.*\", i, flags=re.I)][0]\n",
    "elements.remove(result)\n",
    "number, date = result.split(\"/\")\n",
    "\n",
    "# 3. extra info\n",
    "result = [i for i in elements if re.match(\"^SEC v\\..*|^Securities and Exchange Commission v\\..*\", i, flags=re.I)][0]\n",
    "elements.remove(result)\n",
    "extra_info = result\n",
    "\n",
    "# 4. 標題\n",
    "title = \" \".join(elements)\n",
    "\n",
    "# 取得內文 => 皆放在p標籤裡面，抓取全部並用join組成一個大字串，再以特定句或是符號進行split\n",
    "articles = \" \".join([i.text for i in ele_soup.select(\"p\")])\n",
    "\n",
    "if \"For further information\" in articles:\n",
    "    contents = articles.split(r\"For further information\")[0]\n",
    "elif \"SEC Complaint\" in articles:\n",
    "    contents = articles.split(\"SEC Complaint\")[0]\n",
    "else:\n",
    "    contents = articles.split(\"\\n\")[0]\n",
    "\n",
    "print(contents)\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[\"LITIGATION RELEASE NO. 18322 / SEPTEMBER 4, 2003\\n\\nSEC BRINGS ENFORCEMENT ACTIONS AGAINST THREE INDIVIDUALS, GOLDMAN SACHS, AND MASSACHUSETTS FINANCIAL SERVICES COMPANY RELATED TO TRADING BASED ON NON-PUBLIC INFORMATION ABOUT THE TREASURY'S DECISION TO CEASE ISSUANCE OF THE 30-YEAR BOND \\n\\nSECURITIES AND EXCHANGE COMMISSION v. PETER J. DAVIS, JR., JOHN M. YOUNGDAHL and STEVEN E. NOTHERN (United States District Court for the Southern District of New York, Civil Action No. 03-CV6672(NRB))\"]\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "IndexError",
     "evalue": "list index out of range",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-567-94dfded0abbe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;31m# 3. extra info\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0melements\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"^SEC v\\..*|^Securities and Exchange Commission v\\..*\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mI\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0melements\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0mextra_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 必要な情報を取る流れ：\n",
    "- 適用年份：2017~2003（再往前需要使用string方法擷取！）\n",
    "- 將匹配完成的組成list，並刪除不需要的元素\n",
    "- 使用re進行擷取\n",
    "- 問題：在取得內文時，不知為何也會一併取到之前的元素（號碼、標題等）\n",
    "  * ■ 應對方法：抓取基本情報同時，使用extract()將資料萃取出來，如此一來後面內文就不會再抓到它們\n",
    "\n",
    "<br>\n",
    "\n",
    "- 待解決問題：\n",
    "  * 抓取元素後得到一大串string -> Error\n",
    "  * extra_info的地方，抓到不只一個元素時的解決方法"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### String方法 => 效率太差，不好使用，暫時以list為主"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "source": [
    "# 利用string\n",
    "elements_str = \"\\n\".join(elements)\n",
    "print(elements_str)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Litigation Release No. 23747 / February 10, 2017\n",
      "Securities and Exchange Commission v. Shaohua (Michael) Yin, et al., Civil Action No. 17-CV-972 (S.D.N.Y., filed February 10, 2017)\n",
      "SEC Charges Chinese Citizens Who Reaped Massive Profits from Insider Trading on Comcast-DreamWorks Acquisition\n",
      "Obtains Court Order Freezing More Than $29 Million in U.S. Accounts\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "source": [
    "# release號碼、日期\n",
    "litigation_number, date = re.findall(\"^Litigation.*\", elements_str, flags=re.M)[0].split(\" / \")\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "source": [
    "# extra info\n",
    "extra_info = re.findall(\"^SEC v\\..*|^Securities and Exchange Commission v\\..*\", elements_str, flags=re.M) \n",
    "extra_info    "
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['SEC v. Magyar Telekom Plc. and Deutsche Telekom AG, Case No. 11 civ 9646 (S.D.N.Y.)',\n",
       " 'SEC v. Straub, et al., Case No. 11 civ 9645 (S.D.N.Y.)']"
      ]
     },
     "metadata": {},
     "execution_count": 141
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# 標題\n",
    "title = \"\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "source": [
    "test_str = \"\"\"\n",
    "Securities and Exchange Commission v. One or More Unknown Purchasers of,\n",
    "SEC v. Magyar Telekom Plc. and Deutsche Telekom AG, Case No. 11 civ 9646 (S.D.N.Y.)\n",
    "SEC v. Straub, et al., Case No. 11 civ 9645 (S.D.N.Y.)\n",
    "\"\"\"\n",
    "\n",
    "re.findall(r\"^SEC v\\..*|^Securities and Exchange Commission v\\..*\", test_str, flags=re.M) "
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['Securities and Exchange Commission v. One or More Unknown Purchasers of,',\n",
       " 'SEC v. Magyar Telekom Plc. and Deutsche Telekom AG, Case No. 11 civ 9646 (S.D.N.Y.)',\n",
       " 'SEC v. Straub, et al., Case No. 11 civ 9645 (S.D.N.Y.)']"
      ]
     },
     "metadata": {},
     "execution_count": 124
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### List方法 => 主要方法"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 548,
   "source": [
    "#### 測試！！！！\n",
    "url = \"https://www.sec.gov/litigation/litreleases/2016/lr23711.htm\" #\"https://www.sec.gov/litigation/litreleases/2017/lr23994.htm\"\n",
    "\n",
    "headers = {\"user-agent\":\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/93.0.4577.63 Safari/537.36\"}\n",
    "ele_res = requests.get(url, headers=headers)\n",
    "ele_soup = BeautifulSoup(ele_res.text, features=\"html.parser\")\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 549,
   "source": [
    "# 取得基本情報 => 幾乎都存放在h開頭的標籤裡面，透過for loop進行蒐集tag\n",
    "raw_data = []\n",
    "for i in range(1, 4):\n",
    "\n",
    "    data = ele_soup.select(f\"h{i}\")\n",
    "    if data:\n",
    "        for d in data:\n",
    "            raw_data.append(d.extract())\n",
    "\n",
    "# 從tag中取出文字\n",
    "elements = list(map(lambda x:x.text, raw_data))\n",
    "elements"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[<h2>U.S. SECURITIES AND EXCHANGE COMMISSION</h2>, <h2>Litigation Release No. 23711 / December 27, 2016</h2>, <h2><i>Securities and Exchange Commission v. Iat Hong, et al.</i>, Civil Action No. 16 cv 9947 (S.D.N.Y., filed Dec. 27, 2016)</h2>, <h3>Chinese Traders Charged with Trading On Hacked Nonpublic Information Stolen from Two New York-Based Law Firms</h3>, <h3><i>Marks First Time SEC Charges Hacking into Law Firm Computer Networks</i></h3>]\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['U.S. SECURITIES AND EXCHANGE COMMISSION',\n",
       " 'Litigation Release No. 23711 / December 27, 2016',\n",
       " 'Securities and Exchange Commission v. Iat Hong, et al., Civil Action No. 16 cv 9947 (S.D.N.Y., filed Dec. 27, 2016)',\n",
       " 'Chinese Traders Charged with Trading On Hacked Nonpublic Information Stolen from Two New York-Based Law Firms',\n",
       " 'Marks First Time SEC Charges Hacking into Law Firm Computer Networks']"
      ]
     },
     "metadata": {},
     "execution_count": 549
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 550,
   "source": [
    "# 利用list\n",
    "elements_list = elements.copy()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 551,
   "source": [
    "elements_list"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['U.S. SECURITIES AND EXCHANGE COMMISSION',\n",
       " 'Litigation Release No. 23711 / December 27, 2016',\n",
       " 'Securities and Exchange Commission v. Iat Hong, et al., Civil Action No. 16 cv 9947 (S.D.N.Y., filed Dec. 27, 2016)',\n",
       " 'Chinese Traders Charged with Trading On Hacked Nonpublic Information Stolen from Two New York-Based Law Firms',\n",
       " 'Marks First Time SEC Charges Hacking into Law Firm Computer Networks']"
      ]
     },
     "metadata": {},
     "execution_count": 551
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 552,
   "source": [
    "useless = [i for i in elements if re.match(\".*Securities and Exchange Commission.*\", i, flags=re.I)][0]\n",
    "elements_list.remove(useless)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 553,
   "source": [
    "# 號碼、日期\n",
    "result = [i for i in elements_list if re.match(\"^Litigation.*\", i, flags=re.I)][0]\n",
    "elements_list.remove(result)\n",
    "number, date = result.split(\"/\")\n",
    "number, date"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "('Litigation Release No. 23711 ', ' December 27, 2016')"
      ]
     },
     "metadata": {},
     "execution_count": 553
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 554,
   "source": [
    "# extra info => 存在兩個sec v.的狀況並被視為不同元素時處理待補\n",
    "result = [i for i in elements_list if re.match(\"^SEC v\\..*|^Securities and Exchange Commission v\\..*\", i, flags=re.I)][0]\n",
    "elements_list.remove(result)\n",
    "extra_info = result\n",
    "extra_info"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'Securities and Exchange Commission v. Iat Hong, et al., Civil Action No. 16 cv 9947 (S.D.N.Y., filed Dec. 27, 2016)'"
      ]
     },
     "metadata": {},
     "execution_count": 554
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 555,
   "source": [
    "# title\n",
    "title = \" \".join(elements_list)\n",
    "title"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'Chinese Traders Charged with Trading On Hacked Nonpublic Information Stolen from Two New York-Based Law Firms Marks First Time SEC Charges Hacking into Law Firm Computer Networks'"
      ]
     },
     "metadata": {},
     "execution_count": 555
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 557,
   "source": [
    "# content \n",
    "articles = \" \".join([i.text for i in ele_soup.select(\"p\")])\n",
    "\n",
    "if \"For further information\" in articles:\n",
    "    contents = articles.split(\"For further information\")[0]\n",
    "elif \"SEC Complaint\" in articles:\n",
    "    contents = articles.split(\"SEC Complaint\")[0]\n",
    "else:\n",
    "    pass\n",
    "\n",
    "print(contents)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The Securities and Exchange Commission today charged three Chinese traders with fraudulently trading on hacked nonpublic market moving information stolen from two prominent New York-based law firms, racking up almost $3 million in illegal profits.  The SEC is also seeking an asset freeze that prevents the traders from cashing in on their illicit gains.  Today's action marks the first time the SEC has charged hacking into a law firm's computer network. The SEC's complaint alleges that Iat Hong, Bo Zheng, and Hung Chin executed a deceptive scheme to hack into the networks of two law firms and steal confidential information pertaining to firm clients that were considering mergers or acquisitions. According to the SEC's complaint, the alleged hacking incidents involved installing malware on the law firms' networks, compromising accounts that enabled access to all email accounts at the firms, and copying and transmitting dozens of gigabytes of emails to remote internet locations.  Defendants Hong and Zheng in particular coveted the emails of attorneys involved in mergers and acquisitions, as they exchanged a list of partners who performed the work at one of the law firms prior to the hack at that firm. In a parallel action, the U.S. Attorney's Office for the Southern District of New York today announced criminal charges. According to the SEC's complaint, Hong, Zheng, and Chin used the stolen confidential information contained in emails to purchase shares in at least three public companies ahead of public announcements about entering into merger agreements.  The SEC alleges that they spent approximately $7.5 million in a one-month period buying shares in semiconductor company Altera Inc. in advance of a 2015 report that it was in talks to be acquired by Intel Corporation.  Within 12 hours of emails being extracted from one of the firms, Hong and Chin allegedly began purchasing shares of e-commerce company Borderfree so aggressively that they accounted for at least 25 percent of the company's trading volume on certain days in advance of the announcement of a 2015 deal.  Hong and Zheng also allegedly traded in advance of a 2014 merger announcement involving InterMune, a pharmaceutical company. The SEC's complaint charges Hong, Zheng, and Chin with violating the antifraud provisions of the federal securities laws and related rules.  The SEC seeks a final judgment ordering them to pay penalties and disgorge ill-gotten gains plus interest and permanently enjoining them from violating the federal securities laws.  Hong's mother is named as a relief defendant in the SEC's complaint for the purpose of recovering ill-gotten gains in her accounts resulting from her son's illicit trading. The SEC's investigation is continuing, and is being conducted by Jennie B. Krasner, Devon Leppink Staren, and staff in the SEC's Information Technology Forensics Group with assistance from Wendy Kong.Â  The case is being supervised by Ricky Sachar and Antonia Chion and the litigation is being led by Britt Biles.Â  The SEC appreciates the assistance of the U.S. Attorney's Office for Southern District of New York, Federal Bureau of Investigation, Hong Kong Securities and Futures Commission, and Financial Industry Regulatory Authority. \n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.3",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.3 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "7b4db6f4314de8d4cd797f28a178c3ff3d1b87f4cb8353b93947a436002fe6e1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}