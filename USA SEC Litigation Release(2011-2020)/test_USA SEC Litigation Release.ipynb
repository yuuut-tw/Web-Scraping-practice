{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## import packages "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "import requests\n",
    "import re\n",
    "import pandas as pd\n",
    "import time\n",
    "import random\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.support.ui import Select\n",
    "from fake_useragent import UserAgent"
   ],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-18T13:28:43.003528Z",
     "start_time": "2021-05-18T13:28:41.238842Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Create functions "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Get articles urls of whole year \n",
    "找出整年度各個article網址"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "def yaerly_url_lists(year, soup):\n",
    "    \n",
    "    # 先將擁有a的標籤留下來，再創造一個url字典 (\"LRN\": 完整url)\n",
    "    if year < 2017 and year >= 2003:\n",
    "        url_data = soup.select(\"tr a\")\n",
    "    elif year >= 2017:\n",
    "        url_data = soup.select(\"table[id='mainlist'] a\")\n",
    "        \n",
    "    try:\n",
    "        url_dict = {i.text: \"https://www.sec.gov\" + i[\"href\"] for i in url_data if re.match(\"^LR\", i.text)}\n",
    "        return url_dict\n",
    "    \n",
    "    except IndexError as e:\n",
    "        print(\"IndexError!\")"
   ],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-18T13:28:44.953329Z",
     "start_time": "2021-05-18T13:28:44.940675Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Get elements of each articles\n",
    "1. 取得各篇articles的元素(標題、發佈號碼、發文時間、額外資訊、內文)\n",
    "2. 將無法爬取的append到error_articles"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "# For 新版\n",
    "def get_elements_new(year, url_dict):\n",
    "    error_articles = []\n",
    "    output_data = []\n",
    "    # headers = {\"user-agent\":\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/93.0.4577.63 Safari/537.36\"}\n",
    "    ua = UserAgent()\n",
    "    headers = {\"user-agent\": ua.google}\n",
    "\n",
    "    # 從url_dict中取得LRN, Url => 取得LRN號碼用意為錯誤發生時可知道是哪篇無法爬取\n",
    "    for LRN, url in url_dict.items():\n",
    "        print(LRN)\n",
    "        \n",
    "        # 建立beautifulsoup物件\n",
    "        ele_url = url\n",
    "        ele_res = requests.get(ele_url, headers=headers)\n",
    "        ele_soup = BeautifulSoup(ele_res.text, features=\"html.parser\")\n",
    "\n",
    "        # 取得 title, release_num, time, extra_info, content => use try-except to filter the error message\n",
    "        try:\n",
    "            title = ele_soup.select(\"h1.alphaheads\")[0].text\n",
    "            subtitle_data = ele_soup.select(\"h2.alphaheads\")\n",
    "            release_num, date = subtitle_data[0].text.split(\"/\")\n",
    "            extra_info = \",\".join([i.text for i in subtitle_data[1:]])\n",
    "            content_data = ele_soup.select('div[id=main-content]')\n",
    "            content = \"\\n\".join([i.text for i in content_data[0].select(\"p\")])\n",
    "\n",
    "            # 將每篇article資料放入輸出資料，待會轉成dataframe\n",
    "            output_data.append([title, release_num, date, extra_info, content])\n",
    "\n",
    "        # 跳過indexerror => append進入error_articles、回傳LRN編號\n",
    "        except (IndexError, ValueError) as e:\n",
    "            error_articles.append((LRN, url))\n",
    "            print(\"有問題無法爬取 => \", LRN)\n",
    "        \n",
    "        time.sleep(1)\n",
    "    # 將當年度error_article依年份存入whole_error_articles中        \n",
    "    whole_error_articles[year] = error_articles\n",
    "    \n",
    "    return output_data"
   ],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-18T13:29:57.634735Z",
     "start_time": "2021-05-18T13:29:57.599699Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "# For 舊版\n",
    "def get_elements_old(year, url_dict):\n",
    "    error_articles = []\n",
    "    output_data = []\n",
    "    ua = UserAgent()\n",
    "    headers = {\"user-agent\": ua.google}\n",
    "\n",
    "    ###### 隨機選取幾篇進行測試\n",
    "    url_random = random.sample(url_dict.keys(), 30)\n",
    "    \n",
    "    # 從url_dict中取得LRN, Url => 取得LRN號碼用意為錯誤發生時可知道是哪篇無法爬取\n",
    "    # for LRN, url in url_dict.items():\n",
    "    for LRN in url_random:\n",
    "        print(LRN)\n",
    "        url = url_dict[LRN]\n",
    "\n",
    "        # 建立beautifulsoup物件\n",
    "        ele_res = requests.get(url, headers=headers)\n",
    "        ele_soup = BeautifulSoup(ele_res.text, features=\"html.parser\")\n",
    "\n",
    "        # 取得 title, release_num, time, extra_info, content => use try-except to filter the error message\n",
    "        try:\n",
    "            # 取得基本情報 => 幾乎都存放在h開頭的標籤裡面，透過for loop進行蒐集tag之後，從soup中extract出來，避免擷取內文時被再次抓到（2011以前都是如此）\n",
    "            raw_data = []\n",
    "            for i in range(1, 4):\n",
    "                data = ele_soup.select(f\"h{i}\")\n",
    "                if data:\n",
    "                    for d in data:\n",
    "                        raw_data.append(d.extract())\n",
    "\n",
    "            # 從tag中取出文字\n",
    "            elements = list(map(lambda x: x.text, raw_data))\n",
    "\n",
    "            # 流程：將必要資訊拿出後，從list裡面remove掉，到最後剩下的element組成titile(無規則性，後續再進行文字處理)\n",
    "            # 1. 找出不必要資訊並刪除\n",
    "            useless = [i for i in elements if re.match(\".*Securities and Exchange Commission.*\", i, flags=re.I)][0]\n",
    "            elements.remove(useless)\n",
    "        \n",
    "            # 2. 號碼、日期\n",
    "            result = [i for i in elements if re.match(\"^Litigation.*\", i, flags=re.I)][0]\n",
    "            elements.remove(result)\n",
    "            release_num, date = result.split(\"/\")\n",
    "\n",
    "            # 3. extra info\n",
    "            result = [i for i in elements if re.match(\"^SEC v\\..*|^Securities and Exchange Commission v\\..*\", i, flags=re.I)][0]\n",
    "            elements.remove(result)\n",
    "            extra_info = result\n",
    "\n",
    "            # 4. 標題\n",
    "            title = \" \".join(elements)\n",
    "\n",
    "            # 5. 取得內文 => 皆放在p標籤裡面，抓取全部並用join組成一個大字串，再以特定句進行split\n",
    "            articles = \" \".join([i.text for i in ele_soup.select(\"p\")])\n",
    "\n",
    "            if \"For further information\" in articles:\n",
    "                content = articles.split(r\"For further information\")[0]\n",
    "            elif \"SEC Complaint\" in articles:\n",
    "                content = articles.split(\"SEC Complaint\")[0]\n",
    "            else:\n",
    "                content = articles\n",
    "\n",
    "            # 將每篇article資料放入輸出資料，待會轉成dataframe\n",
    "            output_data.append([title, release_num, date, extra_info, content])\n",
    "\n",
    "        # 跳過indexerror => append進入error_articles、回傳LRN編號\n",
    "        except (IndexError, ValueError) as e:\n",
    "            error_articles.append((LRN, url))\n",
    "            print(\"有問題無法爬取 => \", LRN)\n",
    "        \n",
    "    # 將當年度error_article依年份存入whole_error_articles中        \n",
    "    whole_error_articles[year] = error_articles\n",
    "    \n",
    "    return output_data"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## starting scrapying\n",
    "\n",
    "<font color=\"red\">注意: 主頁面格式&文章版面依據年份而不同，因此透過年份來判別使用哪個標籤進行爬取，目前越前面年份越容易出錯，仍有很大改善空間!</font>\n",
    "\n",
    "輸出 => excel檔案，包含各年份的sheet"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "# 開始時間\n",
    "start_time = time.time()\n",
    "\n",
    "# 建立whole_error_articles字典存放無法爬取article\n",
    "whole_error_articles = {}\n",
    "\n",
    "# 用來接取year與df\n",
    "df_lists = list()\n",
    "for year in range(2003, 2015):\n",
    "    print(year, \"---\"*20)\n",
    "\n",
    "    # 每年度新聞稿網址略有不同: 2020以後 -> htm, 2020以前 -> shtml\n",
    "    if year < 2020:\n",
    "        main_url = f\"https://www.sec.gov/litigation/litreleases/litrelarchive/litarchive{year}.shtml\"\n",
    "    else:\n",
    "        main_url = f\"https://www.sec.gov/litigation/litreleases/litrelarchive/litarchive{year}.htm\"\n",
    "\n",
    "    # 建立身分認證資料\n",
    "    ua = UserAgent()\n",
    "    headers = {\"user-agent\" : ua.google}\n",
    "\n",
    "    # 加入headless模式 => 注意：使用時google版本可能不一樣\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument(\"--headless\")\n",
    "    options.add_argument(ua.google) # headers\n",
    "\n",
    "    # 將參數帶入\n",
    "    driver = webdriver.Chrome(options=options, executable_path=\"../chromedriver\")\n",
    "    driver.get(main_url)\n",
    "    time.sleep(2)\n",
    "\n",
    "    # 取得頁面的html\n",
    "    main_page_soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "\n",
    "    # 透過yearly_url_lists函數取得該年度所有新聞稿網址，再丟入get_elements函數中取得文章所有資料\n",
    "    yearly_urls = yaerly_url_lists(year, main_page_soup)\n",
    "\n",
    "    # 依照年份選擇使用functions\n",
    "    if year >= 2018: # new\n",
    "        yearly_LR_output = get_elements_new(year, yearly_urls)\n",
    "    else: # old(until 2003)\n",
    "        yearly_LR_output = get_elements_old(year, yearly_urls)\n",
    "    \n",
    "    # 將yearly_LR_output轉成df放入pandas，並寫入csv檔中\n",
    "    df = pd.DataFrame(yearly_LR_output, columns=[\"title\", \"release_num\", \"time\", \"extra_info\", \"content\"])\n",
    "    df_lists.append((year, df))\n",
    "    \n",
    "### 儲存為excel格式\n",
    "# 將每年df APPEND到一個excel檔案裡面，每年的資料以年份當作頁面名稱，如果此資料夾不存在，照理會自動建立，但這台電腦好像無法~\n",
    "with pd.ExcelWriter(\"./test0905.xlsx\", mode=\"w\") as writer:\n",
    "    for d in df_lists:\n",
    "        d[1].to_excel(writer, index=False, sheet_name=f'{d[0]}')\n",
    "\n",
    "# 結束時間\n",
    "end_time = time.time()\n",
    "\n",
    "# 總花費時間\n",
    "print(\"總花費時間: \", end_time - start_time)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "2003 ------------------------------------------------------------\n",
      "LR-18280\n",
      "LR-18322\n",
      "有問題無法爬取 =>  LR-18322\n",
      "LR-18220\n",
      "LR-18167\n",
      "LR-18193\n",
      "LR-18487\n",
      "LR-18519\n",
      "有問題無法爬取 =>  LR-18519\n",
      "LR-18438\n",
      "LR-18131\n",
      "有問題無法爬取 =>  LR-18131\n",
      "LR-17969\n",
      "LR-18398\n",
      "有問題無法爬取 =>  LR-18398\n",
      "LR-18214\n",
      "LR-17942\n",
      "LR-18015\n",
      "LR-17984\n",
      "有問題無法爬取 =>  LR-17984\n",
      "LR-17940\n",
      "有問題無法爬取 =>  LR-17940\n",
      "LR-18148\n",
      "LR-18135\n",
      "有問題無法爬取 =>  LR-18135\n",
      "LR-18479\n",
      "LR-18406\n",
      "LR-18399\n",
      "LR-18007\n",
      "有問題無法爬取 =>  LR-18007\n",
      "LR-18304\n",
      "LR-18149\n",
      "LR-17920\n",
      "有問題無法爬取 =>  LR-17920\n",
      "LR-18428\n",
      "LR-18053\n",
      "有問題無法爬取 =>  LR-18053\n",
      "LR-18093\n",
      "有問題無法爬取 =>  LR-18093\n",
      "LR-18414\n",
      "LR-18192\n",
      "2004 ------------------------------------------------------------\n",
      "LR-18657\n",
      "LR-18846\n",
      "LR-18587\n",
      "有問題無法爬取 =>  LR-18587\n",
      "LR-18919\n",
      "LR-18709\n",
      "LR-18853\n",
      "LR-18885\n",
      "LR-18990\n",
      "LR-18851\n",
      "LR-19001\n",
      "LR-18916\n",
      "LR-18826\n",
      "LR-18857\n",
      "LR-18834A\n",
      "LR-18807\n",
      "LR-18537\n",
      "LR-19009\n",
      "LR-18792\n",
      "LR-18830\n",
      "LR-18641\n",
      "LR-18981\n",
      "LR-18633\n",
      "LR-18768\n",
      "LR-18538\n",
      "有問題無法爬取 =>  LR-18538\n",
      "LR-18733A\n",
      "LR-18822\n",
      "LR-18556\n",
      "LR-18991\n",
      "LR-18955\n",
      "LR-18683\n",
      "2005 ------------------------------------------------------------\n",
      "LR-19163\n",
      "LR-19141\n",
      "LR-19344\n",
      "LR-19186\n",
      "LR-19251\n",
      "LR-19470\n",
      "LR-19488\n",
      "有問題無法爬取 =>  LR-19488\n",
      "LR-19153\n",
      "LR-19095\n",
      "LR-19065\n",
      "LR-19240\n",
      "有問題無法爬取 =>  LR-19240\n",
      "LR-19161\n",
      "LR-19414\n",
      "有問題無法爬取 =>  LR-19414\n",
      "LR-19443\n",
      "LR-19078\n",
      "LR-19489\n",
      "有問題無法爬取 =>  LR-19489\n",
      "LR-19274\n",
      "LR-19442\n",
      "有問題無法爬取 =>  LR-19442\n",
      "LR-19040\n",
      "LR-19306\n",
      "LR-19392\n",
      "LR-19103\n",
      "LR-19329\n",
      "LR-19222\n",
      "LR-19281\n",
      "LR-19174\n",
      "有問題無法爬取 =>  LR-19174\n",
      "LR-19187\n",
      "LR-19092\n",
      "LR-19467\n",
      "LR-19422\n",
      "2006 ------------------------------------------------------------\n",
      "LR-19923\n",
      "LR-19770\n",
      "有問題無法爬取 =>  LR-19770\n",
      "LR-19534\n",
      "LR-19639\n",
      "LR-19817\n",
      "LR-19615\n",
      "LR-19854\n",
      "LR-19612\n",
      "LR-19949\n",
      "LR-19807\n",
      "LR-19718\n",
      "LR-19753\n",
      "有問題無法爬取 =>  LR-19753\n",
      "LR-19550\n",
      "LR-19748\n",
      "LR-19756\n",
      "LR-19680\n",
      "LR-19648\n",
      "LR-19840\n",
      "LR-19575\n",
      "LR-19540\n",
      "LR-19665\n",
      "LR-19704\n",
      "LR-19927\n",
      "LR-19696\n",
      "LR-19678\n",
      "LR-19938\n",
      "LR-19698\n",
      "LR-19771\n",
      "有問題無法爬取 =>  LR-19771\n",
      "LR-19933\n",
      "LR-19520\n",
      "有問題無法爬取 =>  LR-19520\n",
      "2007 ------------------------------------------------------------\n",
      "LR-20336\n",
      "LR-20127\n",
      "LR-20089\n",
      "LR-20223\n",
      "LR-20366\n",
      "LR-20241\n",
      "LR-20238\n",
      "有問題無法爬取 =>  LR-20238\n",
      "LR-20042\n",
      "LR-20063\n",
      "有問題無法爬取 =>  LR-20063\n",
      "LR-20019\n",
      "LR-20390\n",
      "LR-20407\n",
      "LR-20165\n",
      "LR-20376\n",
      "LR-20197\n",
      "LR-19981\n",
      "LR-20265\n",
      "LR-20159\n",
      "LR-20404\n",
      "LR-20381\n",
      "LR-20209\n",
      "LR-20155\n",
      "LR-20163\n",
      "LR-20340\n",
      "LR-20284\n",
      "LR-20389\n",
      "LR-20307\n",
      "LR-20277\n",
      "LR-20292\n",
      "LR-20235\n",
      "2008 ------------------------------------------------------------\n",
      "LR-20766\n",
      "有問題無法爬取 =>  LR-20766\n",
      "LR-20608\n",
      "LR-20581\n",
      "LR-20826\n",
      "LR-20494\n",
      "LR-20809\n",
      "LR-20488\n",
      "LR-20447\n",
      "LR-20752\n",
      "LR-20485\n",
      "LR-20555\n",
      "LR-20424\n",
      "LR-20459\n",
      "LR-20812\n",
      "LR-20571\n",
      "LR-20652\n",
      "LR-20478\n",
      "LR-20524\n",
      "LR-20517\n",
      "LR-20601\n",
      "LR-20660\n",
      "有問題無法爬取 =>  LR-20660\n",
      "LR-20791\n",
      "LR-20472\n",
      "LR-20816A\n",
      "LR-20806\n",
      "LR-20637\n",
      "LR-20817\n",
      "LR-20783\n",
      "LR-20776\n",
      "有問題無法爬取 =>  LR-20776\n",
      "LR-20457\n",
      "2009 ------------------------------------------------------------\n",
      "LR-20843\n",
      "LR-21173\n",
      "LR-21012\n",
      "LR-21144\n",
      "LR-21166\n",
      "LR-21260\n",
      "LR-20917\n",
      "LR-21077\n",
      "LR-20884\n",
      "LR-21195\n",
      "LR-21268\n",
      "LR-21142\n",
      "LR-21290\n",
      "LR-21315\n",
      "LR-21071\n",
      "LR-21046\n",
      "LR-21270\n",
      "LR-21246\n",
      "LR-21152\n",
      "LR-21057\n",
      "LR-21103\n",
      "LR-21126\n",
      "LR-21350\n",
      "LR-20991\n",
      "LR-21227\n",
      "LR-21331\n",
      "LR-21054\n",
      "LR-20914\n",
      "LR-20995\n",
      "LR-20953A\n",
      "2010 ------------------------------------------------------------\n",
      "LR-21576\n",
      "LR-21521\n",
      "LR-21748\n",
      "LR-21687A\n",
      "LR-21773\n",
      "LR-21425\n",
      "LR-21472\n",
      "LR-21676\n",
      "LR-21700\n",
      "LR-21546\n",
      "LR-21401\n",
      "LR-21721\n",
      "LR-21515\n",
      "LR-21369\n",
      "LR-21505\n",
      "LR-21450\n",
      "有問題無法爬取 =>  LR-21450\n",
      "LR-21541\n",
      "LR-21713\n",
      "LR-21476\n",
      "LR-21441\n",
      "LR-21435\n",
      "LR-21479\n",
      "LR-21552\n",
      "LR-21763\n",
      "LR-21580\n",
      "LR-21462\n",
      "LR-21671\n",
      "LR-21433\n",
      "LR-21391\n",
      "LR-21496\n",
      "2011 ------------------------------------------------------------\n",
      "LR-21956\n",
      "LR-22146A\n",
      "LR-21816\n",
      "LR-22162\n",
      "LR-22145\n",
      "LR-21865\n",
      "LR-22144\n",
      "LR-21798\n",
      "LR-22174\n",
      "LR-22204\n",
      "LR-21856\n",
      "LR-22029\n",
      "LR-22110\n",
      "LR-22207\n",
      "LR-21914\n",
      "LR-22180\n",
      "LR-22169\n",
      "LR-22012\n",
      "LR-22038\n",
      "LR-21971\n",
      "LR-21926\n",
      "LR-22051\n",
      "LR-21924\n",
      "LR-21870\n",
      "LR-22191\n",
      "LR-22052\n",
      "LR-21970\n",
      "LR-22129\n",
      "LR-21900\n",
      "LR-21940\n",
      "2012 ------------------------------------------------------------\n",
      "LR-22244\n",
      "LR-22299\n",
      "LR-22399\n",
      "LR-22516\n",
      "LR-22275\n",
      "LR-22343\n",
      "LR-22418\n",
      "LR-22424\n",
      "LR-22568\n",
      "LR-22544\n",
      "LR-22376\n",
      "LR-22301\n",
      "LR-22356\n",
      "LR-22434\n",
      "有問題無法爬取 =>  LR-22434\n",
      "LR-22335\n",
      "LR-22577\n",
      "LR-22274\n",
      "LR-22425\n",
      "LR-22378\n",
      "LR-22483\n",
      "LR-22510\n",
      "LR-22308\n",
      "LR-22554\n",
      "LR-22326\n",
      "有問題無法爬取 =>  LR-22326\n",
      "LR-22545\n",
      "LR-22290\n",
      "LR-22351\n",
      "LR-22398\n",
      "LR-22280\n",
      "LR-22543\n",
      "有問題無法爬取 =>  LR-22543\n",
      "2013 ------------------------------------------------------------\n",
      "LR-22815\n",
      "LR-22717\n",
      "LR-22849\n",
      "LR-22675\n",
      "LR-22705\n",
      "LR-22773\n",
      "LR-22726\n",
      "LR-22844\n",
      "LR-22809\n",
      "LR-22672\n",
      "LR-22701\n",
      "LR-22837\n",
      "LR-22657\n",
      "LR-22693\n",
      "LR-22746\n",
      "LR-22853\n",
      "LR-22870\n",
      "LR-22702\n",
      "LR-22790\n",
      "LR-22742\n",
      "LR-22816\n",
      "LR-22618\n",
      "LR-22583\n",
      "LR-22718\n",
      "LR-22697\n",
      "LR-22595\n",
      "LR-22753\n",
      "LR-22681\n",
      "LR-22806\n",
      "LR-22733\n",
      "2014 ------------------------------------------------------------\n",
      "LR-22961\n",
      "LR-22993\n",
      "LR-22924\n",
      "LR-22962\n",
      "LR-23154\n",
      "LR-23053\n",
      "LR-23139\n",
      "LR-22936\n",
      "LR-23125\n",
      "LR-23049\n",
      "LR-23002\n",
      "LR-23168\n",
      "LR-23114\n",
      "LR-23035\n",
      "LR-23012\n",
      "LR-22922\n",
      "LR-23022\n",
      "LR-23006\n",
      "LR-23077\n",
      "LR-23144\n",
      "LR-23045\n",
      "LR-23086\n",
      "LR-23150\n",
      "LR-22987\n",
      "LR-22907\n",
      "LR-22969\n",
      "LR-23008\n",
      "LR-23066\n",
      "LR-22996\n",
      "LR-22919\n",
      "總花費時間:  324.240797996521\n"
     ]
    }
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-18T13:30:55.301563Z",
     "start_time": "2021-05-18T13:30:00.800462Z"
    },
    "scrolled": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Check out the articles that can't be scrapyed"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "for key, value in whole_error_articles.items():\n",
    "    print(f'{key}年無法爬取篇數&失敗率: {len(value)}, {round(len(value)/30*100, 2)}%')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "2003年無法爬取篇數&失敗率: 11, 36.67%\n",
      "2004年無法爬取篇數&失敗率: 2, 6.67%\n",
      "2005年無法爬取篇數&失敗率: 6, 20.0%\n",
      "2006年無法爬取篇數&失敗率: 4, 13.33%\n",
      "2007年無法爬取篇數&失敗率: 2, 6.67%\n",
      "2008年無法爬取篇數&失敗率: 3, 10.0%\n",
      "2009年無法爬取篇數&失敗率: 0, 0.0%\n",
      "2010年無法爬取篇數&失敗率: 1, 3.33%\n",
      "2011年無法爬取篇數&失敗率: 0, 0.0%\n",
      "2012年無法爬取篇數&失敗率: 3, 10.0%\n",
      "2013年無法爬取篇數&失敗率: 0, 0.0%\n",
      "2014年無法爬取篇數&失敗率: 0, 0.0%\n"
     ]
    }
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-18T14:16:02.010890Z",
     "start_time": "2021-05-18T14:16:01.991054Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 測試使用"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "source": [
    "## 使用selenium\n",
    "main_url = \"https://www.sec.gov/litigation/litreleases/litrelarchive/litarchive2020.htm\"\n",
    "\n",
    "# 加入headless模式 => 注意：使用時google版本可能不一樣\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument(\"--headless\")\n",
    "ua = UserAgent()\n",
    "options.add_argument(ua.google) # headers\n",
    "\n",
    "# 將參數帶入\n",
    "driver = webdriver.Chrome(options=options, executable_path=\"../chromedriver\")\n",
    "driver.get(main_url)\n",
    "time.sleep(2)\n",
    "\n",
    "# 取得頁面的html\n",
    "main_page_soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "\n",
    "# 透過yearly_url_lists函數取得該年度所有新聞稿網址，再丟入get_elements函數中取得文章所有資料\n",
    "yearly_urls = yaerly_url_lists(2020, main_page_soup)\n",
    "yearly_LR_output = get_elements_new(2020, yearly_urls)\n",
    "\n",
    "yearly_urls\n",
    "# # # # 將yearly_LR_output轉成df放入pandas，並寫入csv檔中\n",
    "df = pd.DataFrame(yearly_LR_output, columns=[\"title\", \"release_num\", \"date\", \"extra_info\", \"content\"])\n",
    "#df_lists.append((2020, df))\n",
    "df.head()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "LR-25003\n",
      "LR-25002\n",
      "LR-25001\n",
      "LR-25000\n",
      "LR-24999\n",
      "LR-24998\n",
      "LR-24997\n",
      "LR-24996\n",
      "LR-24995\n",
      "LR-24994\n",
      "LR-24993\n",
      "LR-24992\n",
      "LR-24991\n",
      "LR-24990\n",
      "LR-24989\n",
      "LR-24988\n",
      "LR-24987\n",
      "LR-24986\n",
      "LR-24985\n",
      "LR-24984\n",
      "LR-24983\n",
      "LR-24982\n",
      "LR-24981\n",
      "LR-24980\n",
      "LR-24979\n",
      "LR-24978\n",
      "LR-24977\n",
      "LR-24976\n",
      "有問題無法爬取 =>  LR-24976\n",
      "LR-24975\n",
      "LR-24974\n",
      "LR-24973\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-3c28c16dfe57>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m# 透過yearly_url_lists函數取得該年度所有新聞稿網址，再丟入get_elements函數中取得文章所有資料\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0myearly_urls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0myaerly_url_lists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2020\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmain_page_soup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0myearly_LR_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_elements_new\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2020\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myearly_urls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0myearly_urls\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-a6b9414cc9de>\u001b[0m in \u001b[0;36mget_elements_new\u001b[0;34m(year, url_dict)\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"有問題無法爬取 => \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLRN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m     \u001b[0;31m# 將當年度error_article依年份存入whole_error_articles中\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0mwhole_error_articles\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0myear\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0merror_articles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.3 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "interpreter": {
   "hash": "7b4db6f4314de8d4cd797f28a178c3ff3d1b87f4cb8353b93947a436002fe6e1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}