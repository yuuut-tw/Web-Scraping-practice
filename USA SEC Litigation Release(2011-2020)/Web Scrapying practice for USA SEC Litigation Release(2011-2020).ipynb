{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import packages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-18T13:28:43.003528Z",
     "start_time": "2021-05-18T13:28:41.238842Z"
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create functions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get articles urls of whole year \n",
    "找出整年度各個article網址"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-18T13:28:44.953329Z",
     "start_time": "2021-05-18T13:28:44.940675Z"
    }
   },
   "outputs": [],
   "source": [
    "def yaerly_url_lists(year, soup):\n",
    "    \n",
    "    # 先將擁有a的標籤留下來，再創造一個url字典 (\"LRN\": 完整url)\n",
    "    if year < 2017 and year >= 2011:\n",
    "        url_data = soup.select(\"tr a\")\n",
    "    elif year >= 2017:\n",
    "        url_data = soup.select(\"table[id=mainlist] a\")\n",
    "        \n",
    "    try:\n",
    "        url_dict = {i.text: \"https://www.sec.gov\" + i[\"href\"] for i in url_data if re.match(\"^LR\", i.text)}\n",
    "        return url_dict\n",
    "    \n",
    "    except IndexError as e:\n",
    "        print(\"IndexError!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get elements of each articles\n",
    "1. 取得各篇articles的元素(標題、發佈號碼、發文時間、額外資訊、內文)\n",
    "2. 將無法爬取的append到error_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-18T13:29:57.634735Z",
     "start_time": "2021-05-18T13:29:57.599699Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_elements(year, url_dict):\n",
    "    error_articles = []\n",
    "    output_data = []\n",
    "    \n",
    "    # 從url_dict中取得LRN, Url => 取得LRN號碼用意為錯誤發生時可知道是哪篇無法爬取\n",
    "    for LRN, url in url_dict.items():\n",
    "        print(LRN)\n",
    "        \n",
    "        # 建立beautifulsoup物件\n",
    "        ele_url = url\n",
    "        ele_res = requests.get(ele_url, headers=headers)\n",
    "        ele_soup = BeautifulSoup(ele_res.text, features=\"html.parser\")\n",
    "\n",
    "        # 取得 title, release_num, time, extra_info, content => use try-except to filter the error message\n",
    "        try:\n",
    "            if 2011 <= year <= 2017:\n",
    "                title = ele_soup.select(\"h3\")[0].text\n",
    "                subtitle_data = ele_soup.select(\"h2\")\n",
    "                release_num, time = list(map(lambda x: x.strip(\" \"), subtitle_data[1].text.split(\"/\")))\n",
    "                extra_info = \",\".join([i.text for i in subtitle_data[1:]])\n",
    "                content_data = ele_soup.select('p')[:-1]\n",
    "                content = \"\\n\".join([i.text for i in content_data])\n",
    "                \n",
    "            elif 2018 <= year:                \n",
    "                title = ele_soup.select(\"h1.alphaheads\")[0].text\n",
    "                subtitle_data = ele_soup.select(\"h2.alphaheads\")\n",
    "                release_num, time = subtitle_data[0].text.split(\"/\")\n",
    "                extra_info = \",\".join([i.text for i in subtitle_data[1:]])\n",
    "                content_data = ele_soup.select('div[id=main-content]')\n",
    "                content = \"\\n\".join([i.text for i in content_data[0].select(\"p\")])\n",
    "\n",
    "            # 將每篇article資料放入輸出資料，待會轉成dataframe\n",
    "            output_data.append([title, release_num, time, extra_info, content])\n",
    "\n",
    "        # 跳過indexerror => append進入error_articles、回傳LRN編號\n",
    "        except (IndexError, ValueError) as e:\n",
    "            error_articles.append((LRN, url))\n",
    "            print(\"有問題無法爬取 => \", LRN)\n",
    "    \n",
    "    # 將當年度error_article依年份存入whole_error_articles中        \n",
    "    whole_error_articles[year] = error_articles\n",
    "    \n",
    "    return output_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## starting scrapying\n",
    "\n",
    "<font color=\"red\">注意: 主頁面格式&文章版面依據年份而不同，因此透過年份來判別使用哪個標籤進行爬取，目前越前面年份越容易出錯，仍有很大改善空間!</font>\n",
    "\n",
    "輸出 => excel檔案，包含各年份的sheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-18T13:30:55.301563Z",
     "start_time": "2021-05-18T13:30:00.800462Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 開始時間\n",
    "start_time = time.time()\n",
    "\n",
    "# 建立whole_error_articles字典存放無法爬取article\n",
    "whole_error_articles = {}\n",
    "\n",
    "df_lists = list()\n",
    "for year in range(2011, 2021):\n",
    "    print(year, \"---\"*20)\n",
    "\n",
    "    # 每年度新聞稿網址略有不同: 2020以後 -> htm、 2020以前 -> shtml\n",
    "    if year < 2020:\n",
    "        main_url = f\"https://www.sec.gov/litigation/litreleases/litrelarchive/litarchive{year}.shtml\"\n",
    "    else:\n",
    "        main_url = f\"https://www.sec.gov/litigation/litreleases/litrelarchive/litarchive{year}.htm\"\n",
    "\n",
    "    # 建立身分認證資料\n",
    "    UserAgent = \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0.4430.93 Safari/537.36\"\n",
    "    headers = {\"User-Agent\" : UserAgent}\n",
    "\n",
    "    # 開始爬蟲\n",
    "    res = requests.get(main_url, headers=headers)\n",
    "    main_page_soup = BeautifulSoup(res.text, features=\"html.parser\")\n",
    "    \n",
    "    # 透過yearly_url_lists函數取得該年度所有新聞稿網址，再丟入get_elements函數中取得文章所有資料\n",
    "    yearly_urls = yaerly_url_lists(year, main_page_soup)\n",
    "    yearly_LR_output = get_elements(year, yearly_urls)\n",
    "\n",
    "    # 將yearly_LR_output轉成df放入pandas，並寫入csv檔中\n",
    "    df = pd.DataFrame(yearly_LR_output, columns=[\"title\", \"release_num\", \"time\", \"extra_info\", \"content\"])\n",
    "    df_lists.append((year, df))\n",
    "    \n",
    "### 儲存為excel格式\n",
    "# 將每年df APPEND到一個excel檔案裡面，每年的資料以年份當作頁面名稱，如果此資料夾不存在，照理會自動建立，但這台電腦好像無法~\n",
    "with pd.ExcelWriter(\"./test0518(2005~2010).xlsx\", mode=\"w\") as writer:\n",
    "    for d in df_lists:\n",
    "        d[1].to_excel(writer, index=False, sheet_name=f'{d[0]}')\n",
    "\n",
    "# 結束時間\n",
    "end_time = time.time()\n",
    "\n",
    "# 總花費時間\n",
    "print(\"總花費時間: \", end_time - start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check out the articles that can't be scrapyed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-18T14:16:02.010890Z",
     "start_time": "2021-05-18T14:16:01.991054Z"
    }
   },
   "outputs": [],
   "source": [
    "for key, value in whole_error_articles.items():\n",
    "    print(f'{key}年無法爬取篇數: {len(value)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
